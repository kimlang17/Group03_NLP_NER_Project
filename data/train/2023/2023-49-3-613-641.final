Large	B
multilingual	I
language	I
models	I
typically	O
share	O
their	O
parameters	O
across	O
all	O
languages	O
,	O
which	O
enables	O
cross-lingual	B
task	O
transfer	O
,	O
but	O
learning	O
can	O
also	O
be	O
hindered	O
when	O
training	O
updates	O
from	O
different	O
languages	O
are	O
in	O
conflict	O
.	O
In	O
this	O
article	O
,	O
we	O
propose	O
novel	O
methods	O
for	O
using	O
language-specific	O
subnetworks	O
,	O
which	O
control	O
cross-lingual	B
parameter	O
sharing	O
,	O
to	O
reduce	O
conflicts	O
and	O
increase	O
positive	O
transfer	O
during	O
fine-tuning	B
.	O
We	O
introduce	O
dynamic	O
subnetworks	O
,	O
which	O
are	O
jointly	O
updated	O
with	O
the	O
model	O
,	O
and	O
we	O
combine	O
our	O
methods	O
with	O
meta-learning	B
,	O
an	O
established	O
,	O
but	O
complementary	O
,	O
technique	O
for	O
improving	O
cross-lingual	B
transfer	O
.	O
Finally	O
,	O
we	O
provide	O
extensive	O
analyses	O
of	O
how	O
each	O
of	O
our	O
methods	O
affects	O
the	O
models	O
.	O
