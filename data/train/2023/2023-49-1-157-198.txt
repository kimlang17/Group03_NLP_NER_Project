Annotated 	 B
data 	 I
is 	 O
an 	 O
essential 	 B
ingredient 	 I
in 	 O
natural 	 B
language 	 I
processing 	 I
for 	 O
training 	 O
and 	 O
evaluating 	 O
machine 	 B
learning 	 I
models 	 I
. 	 O
It 	 O
is 	 O
therefore 	 O
very 	 O
desirable 	 O
for 	 O
the 	 O
annotations 	 O
to 	 O
be 	 O
of 	 O
high 	 B
quality 	 I
. 	 O
Recent 	 O
work 	 O
, 	 O
however 	 O
, 	 O
has 	 O
shown 	 O
that 	 O
several 	 O
popular 	 O
datasets 	 O
contain 	 O
a 	 O
surprising 	 O
number 	 O
of 	 O
annotation 	 B
errors 	 I
or 	 O
inconsistencies 	 B
. 	 O
To 	 O
alleviate 	 O
this 	 O
issue 	 O
, 	 O
many 	 O
methods 	 O
for 	 O
annotation 	 B
error 	 I
detection 	 I
have 	 O
been 	 O
devised 	 O
over 	 O
the 	 O
years 	 O
. 	 O
While 	 O
researchers 	 O
show 	 O
that 	 O
their 	 O
approaches 	 O
work 	 O
well 	 O
on 	 O
their 	 O
newly 	 O
introduced 	 O
datasets 	 O
, 	 O
they 	 O
rarely 	 O
compare 	 O
their 	 O
methods 	 O
to 	 O
previous 	 O
work 	 O
or 	 O
on 	 O
the 	 O
same 	 O
datasets 	 O
. 	 O
This 	 O
raises 	 O
strong 	 O
concerns 	 O
on 	 O
methods 	 O
 	 O
general 	 O
performance 	 O
and 	 O
makes 	 O
it 	 O
difficult 	 O
to 	 O
assess 	 O
their 	 O
strengths 	 O
and 	 O
weaknesses 	 O
. 	 O
We 	 O
therefore 	 O
reimplement 	 O
18 	 O
methods 	 O
for 	 O
detecting 	 O
potential 	 O
annotation 	 B
errors 	 I
and 	 O
evaluate 	 O
them 	 O
on 	 O
9 	 O
English 	 O
datasets 	 O
for 	 O
text 	 B
classification 	 I
as 	 O
well 	 O
as 	 O
token 	 B
and 	 O
span 	 B
labeling 	 I
. 	 O
In 	 O
addition 	 O
, 	 O
we 	 O
define 	 O
a 	 O
uniform 	 O
evaluation 	 O
setup 	 O
including 	 O
a 	 O
new 	 O
formalization 	 O
of 	 O
the 	 O
annotation 	 B
error 	 I
detection 	 I
task 	 O
, 	 O
evaluation 	 O
protocol 	 O
, 	 O
and 	 O
general 	 O
best 	 O
practices 	 O
. 	 O
To 	 O
facilitate 	 O
future 	 O
research 	 O
and 	 O
reproducibility 	 O
, 	 O
we 	 O
release 	 O
our 	 O
datasets 	 O
and 	 O
implementations 	 O
in 	 O
an 	 O
easy 	 O
- 	 O
to 	 O
- 	 O
use 	 O
and 	 O
open 	 O
source 	 O
software 	 O
package.1 	 O
