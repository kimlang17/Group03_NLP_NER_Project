Multilingual	B
sentence	B
encoders	I
have	O
seen	O
much	O
success	O
in	O
cross   B
-   I
lingual	I
model	O
transfer	O
for	O
downstream	B
NLP	B
tasks	O
.	O
The	O
success	O
of	O
this	O
transfer	O
is	O
,	O
however	O
,	O
dependent	O
on	O
the	O
model	O
’s	O
ability	O
to	O
encode	O
the	O
patterns	O
of	O
cross   B
-   I
lingual	I
similarity	O
and	O
variation	O
.	O
Yet	O
,	O
we	O
know	O
relatively	O
little	O
about	O
the	O
properties	O
of	O
individual	B
languages	I
or	O
the	O
general	O
patterns	O
of	O
linguistic	B
variation	O
that	O
the	O
models	O
encode	O
.	O
In	O
this	O
article	O
,	O
we	O
investigate	O
these	O
questions	O
by	O
leveraging	O
knowledge	O
from	O
the	O
field	O
of	O
linguistic	B
typology	O
,	O
which	O
studies	O
and	O
documents	B
structural	I
and	O
semantic	B
variation	I
across	O
languages	O
.	O
We	O
propose	O
methods	O
for	O
separating	O
language-specific	B
subspaces	I
within	O
state   B
-   I
of  I
-   I
the I
-   I
art	I
multilingual	B
sentence	I
encoders	I
(   O	O
LASER	B
,	O
M   B
-   I
BERT	I
,	O
XLM	B
,	O
and	O
XLM B
-   I
R	I
)	O
with	O
respect	O
to	O
a	O
range	O
of	O
typological	B
properties	O
pertaining	B
to	O
lexical	B
,	O
morphological	B
,	O
and	O
syntactic	B
structure	I
.	O
Moreover	O
,	O
we	O
investigate	O
how	O
typological	B
information	O
about	O
languages	O
is	O
distributed	O
across	O
all	O
layers	O
of	O
the	O
models	O
.	O
Our	O
results	O
show	O
interesting	O
differences	O
in	O
encoding	B
linguistic	B
variation	O
associated	O
with	O
different	O
pretraining	O
strategies	O
.	O
In	O
addition	O
,	O
we	O
propose	O
a	O
simple	O
method	O
to	O
study	O
how	O
shared	O
typological	B
properties	O
of	O
languages	O
are	O
encoded	O
in	O
two	O
state   O
-   O
of  O
-   O
the O
-   O
art	O
multilingual	B
models  I
—   I
M   I
-   I
BERT	B
and	O
XLM B
-   I
R	I
.	O
The	O
results	O
provide	O
insight	O
into	O
their	O
information B
-   I
sharing	I
mechanisms	O
and	O
suggest	O
that	O
these	O
linguistic	B
properties	O
are	O
encoded	O
jointly	O
across	O
typologically	B
similar	O
languages	O
in	O
these	O
models	O
.	O
