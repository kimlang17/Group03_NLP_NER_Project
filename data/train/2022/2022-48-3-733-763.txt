Transformers	B
are	O
arguably	O
the	O
main	O
workhorse	O
in	O
recent	O
natural	B
language	I
processing	I
research	O
.	O
By	O
definition	O
,	O
a	O
Transformer	B
is	O
invariant	O
with	O
respect	O
to	O
reordering	O
of	O
the	O
input	O
.	O
However	O
,	O
language	O
is	O
inherently	B
sequential	I
and	O
word	O
order	O
is	O
essential	O
to	O
the	O
semantics	B
and	O
syntax	B
of	O
an	O
utterance	O
.	O
In	O
this	O
article	O
,	O
we	O
provide	O
an	O
overview	O
and	O
theoretical	O
comparison	O
of	O
existing	O
methods	O
to	O
incorporate	O
position	O
information	O
into	O
Transformer	B
models	O
.	O
The	O
objectives	O
of	O
this	O
survey	O
are	O
to	O
(	O
1	O
)	O
showcase	O
that	O
position	O
information	O
in	O
Transformer	B
is	O
a	O
vibrant	O
and	O
extensive	O
research	O
area	O
;	O
(	O
2	O
)	O
enable	O
the	O
reader	O
to	O
compare	O
existing	O
methods	O
by	O
providing	O
a	O
unified	O
notation	B
and	O
systematization	O
of	O
different	O
approaches	O
along	O
important	O
model	O
dimensions	O
;	O
(	O
3	O
)	O
indicate	O
what	O
characteristics	O
of	O
an	O
application	O
should	O
be	O
taken	O
into	O
account	O
when	O
selecting	O
a	O
position	O
encoding	O
;	O
and	O
(	O
4	O
)	O
provide	O
stimuli	O
for	O
future	O
research	O
.	O

