Transformer	B
language	I
models	I
have	O
received	O
widespread	O
public	O
attention	O
,	O
yet	O
their	O
generated	B
text	I
is	O
often	O
surprising	O
even	O
to	O
NLP	B
researchers	I
.	O
In	O
this	O
survey	O
,	O
we	O
discuss	O
over	O
250	O
recent	O
studies	O
of	O
English	O
language	I
model	I
behavior	O
before	O
task-specific	B
fine-tuning	I
.	O
Language	B
models	I
possess	I
basic	O
capabilities	O
in	O
syntax	B
,	O
semantics	B
,	O
pragmatics	B
,	O
world	B
knowledge	I
,	O
and	O
reasoning	B
,	O
but	O
these	O
capabilities	O
are	O
sensitive	O
to	O
specific	O
inputs	O
and	O
surface	O
features	O
.	O
Despite	O
dramatic	O
increases	O
in	O
generated	B
text	I
quality	I
as	O
models	B
scale	I
to	O
hundreds	O
of	O
billions	O
of	O
parameters	B
,	O
the	O
models	B
are	O
still	O
prone	O
to	O
unfactual	B
responses	I
,	O
commonsense	B
errors	I
,	O
memorized	B
text	I
,	O
and	O
social	B
biases	I
.	O
Many	O
of	O
these	O
weaknesses	O
can	O
be	O
framed	O
as	O
over-generalizations	B
or	O
under-generalizations	B
of	O
learned	B
patterns	I
in	O
text	O
.	O
We	O
synthesize	O
recent	O
results	O
to	O
highlight	O
what	O
is	O
currently	O
known	O
about	O
large	O
language	B
model	I
capabilities	O
,	O
thus	O
providing	O
a	O
resource	O
for	O
applied	O
work	O
and	O
for	O
research	O
in	O
adjacent	O
fields	O
that	O
use	O
language	B
models	I
.	O