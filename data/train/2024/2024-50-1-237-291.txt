Large	B
language	I
models	I
(	O
LLMs	B
)	O
are	O
capable	O
of	O
successfully	O
performing	O
many	O
language	B
processing	I
tasks	O
zero-shot	B
(	O
without	O
training	B
data	I
)	O
.	O
If	O
zero-shot	B
LLMs	I
can	O
also	O
reliably	O
classify	O
and	O
explain	O
social	O
phenomena	O
like	O
persuasiveness	O
and	O
political	O
ideology	O
,	O
then	O
LLMs	B
could	O
augment	O
the	O
computational	B
social	I
science	I
(	O
CSS	B
)	O
pipeline	B
in	O
important	O
ways	O
.	O
This	O
work	O
provides	O
a	O
road	O
map	O
for	O
using	O
LLMs	B
as	O
CSS	B
tools	O
.	O
Towards	O
this	O
end	O
,	O
we	O
contribute	O
a	O
set	O
of	O
prompting	O
best	O
practices	O
and	O
an	O
extensive	O
evaluation	B
pipeline	I
to	O
measure	O
the	O
zero-shot	B
performance	I
of	O
13	O
language	B
models	I
on	O
25	O
representative	O
English	O
CSS	B
benchmarks	I
.	O
On	O
taxonomic	B
labeling	I
tasks	I
(	O
classification	B
)	O
,	O
LLMs	B
fail	O
to	O
outperform	O
the	O
best	O
fine-tuned	B
models	I
but	O
still	O
achieve	O
fair	O
levels	O
of	O
agreement	O
with	O
humans	O
.	O
On	O
free-form	B
coding	I
tasks	I
(	O
generation	B
)	O
,	O
LLMs	B
produce	O
explanations	O
that	O
often	O
exceed	O
the	O
quality	O
of	O
crowdworkers	O
’	O
gold	O
references	O
.	O
We	O
conclude	O
that	O
the	O
performance	O
of	O
today	O
’	O
s	O
LLMs	B
can	O
augment	O
the	O
CSS	B
research	I
pipeline	I
in	O
two	O
ways	O
:	O
(	O
1	O
)	O
serving	O
as	O
zero-shot	B
data	I
annotators	I
on	O
human	B
annotation	I
teams	O
,	O
and	O
(	O
2	O
)	O
bootstrapping	O
challenging	O
creative	B
generation	I
tasks	I
(	O
e.g.	O
,	O
explaining	O
the	O
underlying	O
attributes	O
of	O
a	O
text	O
)	O
.	O
In	O
summary	O
,	O
LLMs	B
are	O
posed	O
to	O
meaningfully	O
participate	O
in	O
social	B
science	I
analysis	I
in	O
partnership	O
with	O
humans	O
.	O