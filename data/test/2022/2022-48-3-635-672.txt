Multilingual	B
sentence	O
encoders	O
have	O
seen	O
much	O
success	O
in	O
cross-lingual	B
model	O
transfer	O
for	O
downstream	B
NLP	I
tasks	O
.	O
The	O
success	O
of	O
this	O
transfer	O
is	O
,	O
however	O
,	O
dependent	O
on	O
the	O
model	O
’s	O
ability	O
to	O
encode	O
the	O
patterns	O
of	O
cross-lingual	B
similarity	O
and	O
variation	O
.	O
Yet	O
,	O
we	O
know	O
relatively	O
little	O
about	O
the	O
properties	O
of	O
individual	O
languages	O
or	O
the	O
general	O
patterns	O
of	O
linguistic	B
variation	O
that	O
the	O
models	O
encode	O
.	O
In	O
this	O
article	O
,	O
we	O
investigate	O
these	O
questions	O
by	O
leveraging	O
knowledge	O
from	O
the	O
field	O
of	O
linguistic	B
typology	O
,	O
which	O
studies	O
and	O
documents	O
structural	O
and	O
semantic	O
variation	O
across	O
languages	O
.	O
We	O
propose	O
methods	O
for	O
separating	O
language-specific	B
subspaces	O
within	O
state-of-the-art	O
multilingual	B
sentence	I
encoders	O
(   O	O
LASER	B
,	O
M-BERT	B
,	O
XLM	B
,	O
and	O
XLM-R	B
)	O
with	O
respect	O
to	O
a	O
range	O
of	O
typological	B
properties	O
pertaining	O
to	O
lexical	O
,	O
morphological	B
,	O
and	O
syntactic	B
structure	i
.	O
Moreover	O
,	O
we	O
investigate	O
how	O
typological	B
information	O
about	O
languages	O
is	O
distributed	O
across	O
all	O
layers	O
of	O
the	O
models	O
.	O
Our	O
results	O
show	O
interesting	O
differences	O
in	O
encoding	B
linguistic	B
variation	O
associated	O
with	O
different	O
pretraining	O
strategies	O
.	O
In	O
addition	O
,	O
we	O
propose	O
a	O
simple	O
method	O
to	O
study	O
how	O
shared	O
typological	B
properties	O
of	O
languages	O
are	O
encoded	O
in	O
two	O
state-of-the-art	O
multilingual	B
models—M-BERT	B
and	O
XLM-R	B
.	O
The	O
results	O
provide	O
insight	O
into	O
their	O
information-sharing	B
mechanisms	O
and	O
suggest	O
that	O
these	O
linguistic	O
properties	O
are	O
encoded	O
jointly	O
across	O
typologically	B
similar	O
languages	O
in	O
these	O
models	O
.	O
