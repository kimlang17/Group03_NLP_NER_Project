{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure NLTK is ready\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file\n",
    "train_data = pd.read_csv('data/data_train.csv', encoding = \"ISO-8859-1\")\n",
    "test_data = pd.read_csv('data/data_test.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     token label  token_length\n",
      "0       In     O             2\n",
      "1     this     O             4\n",
      "2  article     O             7\n",
      "3       we     O             2\n",
      "4  discuss     O             7\n",
      "         token label  token_length\n",
      "0      Systems     O             7\n",
      "1        based     O             5\n",
      "2           on     O             2\n",
      "3  synchronous     B            11\n",
      "4     grammars     I             8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26742, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(test_data.head())\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26742.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.180353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.361480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token_length\n",
       "count  26742.000000\n",
       "mean       5.180353\n",
       "std        3.361480\n",
       "min        1.000000\n",
       "25%        2.000000\n",
       "50%        4.000000\n",
       "75%        8.000000\n",
       "max       44.000000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'B', 'I'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token           0\n",
       "label           0\n",
       "token_length    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the datasets have a 'sentence_id' column for grouping tokens\n",
    "if 'sentence_id' not in train_data.columns:\n",
    "    train_data['sentence_id'] = (train_data['token'] == '.').cumsum()\n",
    "\n",
    "if 'sentence_id' not in test_data.columns:\n",
    "    test_data['sentence_id'] = (test_data['token'] == '.').cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features for a token\n",
    "def extract_features(df, i):\n",
    "    token = df.iloc[i]['token']\n",
    "    token_length = df.iloc[i]['token_length']\n",
    "    \n",
    "    features = {\n",
    "        'token': token.lower(),\n",
    "        'is_upper': token.isupper(),\n",
    "        'is_title': token.istitle(),\n",
    "        'is_digit': token.isdigit(),\n",
    "        'token_length': token_length,\n",
    "    }\n",
    "    \n",
    "    if i > 0:\n",
    "        prev_token = df.iloc[i - 1]['token']\n",
    "        features.update({\n",
    "            '-1:token': prev_token.lower(),\n",
    "            '-1:is_upper': prev_token.isupper(),\n",
    "            '-1:is_title': prev_token.istitle(),\n",
    "            '-1:is_digit': prev_token.isdigit(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of Sentence\n",
    "\n",
    "    if i < len(df) - 1:\n",
    "        next_token = df.iloc[i + 1]['token']\n",
    "        features.update({\n",
    "            '+1:token': next_token.lower(),\n",
    "            '+1:is_upper': next_token.isupper(),\n",
    "            '+1:is_title': next_token.istitle(),\n",
    "            '+1:is_digit': next_token.isdigit(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of Sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to prepare data for CRF\n",
    "def create_dataset(df):\n",
    "    grouped = df.groupby('sentence_id')  # Group by sentence ID\n",
    "    X, y = [], []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        X.append([extract_features(group, i) for i in range(len(group))])\n",
    "        y.append(group['label'].tolist())\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and labels for train and test datasets\n",
    "X_train, y_train = create_dataset(train_data)\n",
    "X_test, y_test = create_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CRF model\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    ")\n",
    "crf_model = crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = crf_model.predict(X_test)\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')  # Remove 'O' from evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6628394619979354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.67      0.53      0.59       445\n",
      "           I       0.75      0.71      0.73       482\n",
      "\n",
      "   micro avg       0.71      0.62      0.67       927\n",
      "   macro avg       0.71      0.62      0.66       927\n",
      "weighted avg       0.71      0.62      0.66       927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score:\", metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels))\n",
    "print(metrics.flat_classification_report(y_test, y_pred, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "with open(\"crf_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(crf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for inference on raw unseen text\n",
    "def preprocess_raw_text(raw_text):\n",
    "    \"\"\"\n",
    "    Preprocess raw text to prepare it for CRF prediction.\n",
    "    :param raw_text: A string of raw text.\n",
    "    :return: A DataFrame with 'token' and 'token_length' columns.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(raw_text)  # Tokenize the text\n",
    "    token_lengths = [len(token) for token in tokens]\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'token': tokens,\n",
    "        'token_length': token_lengths\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_raw_text(raw_text, model):\n",
    "    \"\"\"\n",
    "    Predict labels for raw text input.\n",
    "    :param raw_text: A string of raw text.\n",
    "    :param model: A trained CRF model.\n",
    "    :return: DataFrame with tokens and their predicted labels.\n",
    "    \"\"\"\n",
    "    data = preprocess_raw_text(raw_text)  # Preprocess raw text\n",
    "    data['sentence_id'] = (data['token'] == '.').cumsum()  # Create sentence IDs\n",
    "    \n",
    "    grouped = data.groupby('sentence_id')\n",
    "    X_unseen = [[extract_features(group, i) for i in range(len(group))] for _, group in grouped]\n",
    "    predictions = model.predict(X_unseen)\n",
    "    \n",
    "    # Add predictions to the DataFrame\n",
    "    data['predicted_label'] = [label for sentence in predictions for label in sentence]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          token predicted_label\n",
      "0          This               O\n",
      "1            is               O\n",
      "2            an               O\n",
      "3       example               O\n",
      "4      sentence               B\n",
      "5           for               O\n",
      "6         named               B\n",
      "7        entity               I\n",
      "8   recognition               I\n",
      "9       testing               O\n",
      "10            .               O\n"
     ]
    }
   ],
   "source": [
    "# Example raw unseen text\n",
    "raw_text = \"This is an example sentence for named entity recognition testing.\"\n",
    "\n",
    "# Predict labels for unseen text\n",
    "result = predict_raw_text(raw_text, crf_model)\n",
    "\n",
    "# Display the result\n",
    "print(result[['token', 'predicted_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            token predicted_label\n",
      "0         Natural               B\n",
      "1        language               I\n",
      "2      processing               I\n",
      "3               (               O\n",
      "4             NLP               B\n",
      "5               )               O\n",
      "6        combines               O\n",
      "7   computational               B\n",
      "8     linguistics               I\n",
      "9               ,               O\n",
      "10        machine               B\n",
      "11       learning               I\n",
      "12              ,               O\n",
      "13            and               O\n",
      "14           deep               B\n",
      "15       learning               I\n",
      "16         models               I\n",
      "17              .               O\n"
     ]
    }
   ],
   "source": [
    "# Example raw unseen text\n",
    "raw_text = \"Natural language processing (NLP) combines computational linguistics, machine learning, and deep learning models.\"\n",
    "\n",
    "# Predict labels for unseen text\n",
    "result = predict_raw_text(raw_text, crf_model)\n",
    "\n",
    "# Display the result\n",
    "print(result[['token', 'predicted_label']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
